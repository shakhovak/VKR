{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:53.494933Z",
     "iopub.status.busy": "2024-05-25T17:48:53.494395Z",
     "iopub.status.idle": "2024-05-25T17:48:58.478765Z",
     "shell.execute_reply": "2024-05-25T17:48:58.478014Z",
     "shell.execute_reply.started": "2024-05-25T17:48:53.494891Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-05-25 17:48:56.504385: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-25 17:48:57.250702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "data = pd.read_json(\"data/train.json\")\n",
    "data = data.drop_duplicates(subset=\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:58.480226Z",
     "iopub.status.busy": "2024-05-25T17:48:58.479926Z",
     "iopub.status.idle": "2024-05-25T17:48:58.495677Z",
     "shell.execute_reply": "2024-05-25T17:48:58.495010Z",
     "shell.execute_reply.started": "2024-05-25T17:48:58.480206Z"
    }
   },
   "outputs": [],
   "source": [
    "def createATE_dataset(sample, prompts_file_path):\n",
    "    \"\"\"function to create ATE dataset for FLAN\"\"\"\n",
    "    with open(prompts_file_path, encoding = 'UTF-8') as fp:\n",
    "        template = json.load(fp)\n",
    "\n",
    "    num = random.randint(1, len(template))\n",
    "    instruction = template[\"ATE\"][str(num)]\n",
    "\n",
    "    sample[\"aspect_list\"] = \",\".join([item[\"word\"] for item in sample[\"entities\"]])\n",
    "    sample[\"ner_list\"] = \",\".join(\n",
    "        [item[\"entity_group\"] for item in sample[\"entities\"]]\n",
    "    )\n",
    "    sample[\"aspect_ner_list\"] = \",\".join(\n",
    "        [f\"{item['word']}:{item['entity_group']}\" for item in sample[\"entities\"]]\n",
    "    )\n",
    "    sample[\"aspect_ner_output\"] = f\"Ответ: \\n{sample['aspect_list']}</s>\"\n",
    "    sample[\"aspect_ner_input\"] = (\n",
    "        f\"<LM>Задача: Извлечение именованных сущностей \\n{instruction}\\n{sample['text']}\\n\"\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:58.496719Z",
     "iopub.status.busy": "2024-05-25T17:48:58.496441Z",
     "iopub.status.idle": "2024-05-25T17:48:58.511259Z",
     "shell.execute_reply": "2024-05-25T17:48:58.510548Z",
     "shell.execute_reply.started": "2024-05-25T17:48:58.496701Z"
    }
   },
   "outputs": [],
   "source": [
    "def createASC_dataset(sample, prompts_file_path):\n",
    "    \"\"\"function to create ASC dataset for FLAN\"\"\"\n",
    "    with open(prompts_file_path, encoding=\"UTF-8\") as fp:\n",
    "        template = json.load(fp)\n",
    "\n",
    "    num = random.randint(1, len(template))\n",
    "    instruction = template[\"ASC\"][str(num)]\n",
    "\n",
    "    sample[\"aspect_list\"] = \",\".join([item[\"word\"] for item in sample[\"entities\"]])\n",
    "    sample[\"ner_list\"] = \",\".join(\n",
    "        [item[\"entity_group\"] for item in sample[\"entities\"]]\n",
    "    )\n",
    "    sample[\"aspect_ner_list\"] = \",\".join(\n",
    "        [f\"{item['word']}:{item['entity_group']}\" for item in sample[\"entities\"]]\n",
    "    )\n",
    "    sample[\"aspect_ner_output\"] = f\"Ответ: \\n{sample['ner_list']}</s>\"\n",
    "    sample[\"aspect_ner_input\"] = (\n",
    "        f\"<LM>Задача: Классификация именованных сущностей \\n{instruction}\\nТекст: \\n{sample['text']}\\nВыделенные именованные сущности: {sample['ner_list']}\\n\"\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:58.513073Z",
     "iopub.status.busy": "2024-05-25T17:48:58.512538Z",
     "iopub.status.idle": "2024-05-25T17:48:58.536694Z",
     "shell.execute_reply": "2024-05-25T17:48:58.536104Z",
     "shell.execute_reply.started": "2024-05-25T17:48:58.513054Z"
    }
   },
   "outputs": [],
   "source": [
    "def createABSA_dataset(sample, prompts_file_path):\n",
    "    \"\"\"function to create ABSA dataset for FLAN\"\"\"\n",
    "    with open(prompts_file_path, encoding=\"UTF-8\") as fp:\n",
    "        template = json.load(fp)\n",
    "\n",
    "    num = random.randint(1, len(template))\n",
    "    instruction = template[\"ABSA\"][str(num)]\n",
    "\n",
    "    sample[\"aspect_list\"] = \",\".join([item[\"word\"] for item in sample[\"entities\"]])\n",
    "    sample[\"ner_list\"] = \",\".join(\n",
    "        [item[\"entity_group\"] for item in sample[\"entities\"]]\n",
    "    )\n",
    "    sample[\"aspect_ner_list\"] = \",\".join(\n",
    "        [f\"{item['word']}:{item['entity_group']}\" for item in sample[\"entities\"]]\n",
    "    )\n",
    "\n",
    "    sample[\"aspect_ner_output\"] = f\"Ответ: \\n{sample['aspect_ner_list']}</s>\"\n",
    "    sample[\"aspect_ner_input\"] = (\n",
    "        f\"<LM>Задача: Извлечение и классификация именованных сущностей \\n{instruction}\\n{sample['text']}\\n\"\n",
    "    )\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:58.537571Z",
     "iopub.status.busy": "2024-05-25T17:48:58.537317Z",
     "iopub.status.idle": "2024-05-25T17:48:59.169047Z",
     "shell.execute_reply": "2024-05-25T17:48:59.168293Z",
     "shell.execute_reply.started": "2024-05-25T17:48:58.537554Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 296/296 [00:00<00:00, 3641.67 examples/s]\n",
      "Map: 100%|██████████| 296/296 [00:00<00:00, 3928.63 examples/s]\n",
      "Map: 100%|██████████| 296/296 [00:00<00:00, 3980.86 examples/s]\n",
      "Map: 100%|██████████| 296/296 [00:00<00:00, 3940.50 examples/s]\n",
      "Map: 100%|██████████| 296/296 [00:00<00:00, 4018.62 examples/s]\n",
      "Map: 100%|██████████| 296/296 [00:00<00:00, 4015.04 examples/s]\n",
      "Map: 100%|██████████| 33/33 [00:00<00:00, 2845.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "dataset = Dataset.from_pandas(data)\n",
    "dataset = dataset.shuffle()\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "ATE_dataset_1 = train_test_split[\"train\"].map(\n",
    "    createATE_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ATE_dataset_2 = train_test_split[\"train\"].map(\n",
    "    createATE_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ASC_dataset_1 = train_test_split[\"train\"].map(\n",
    "    createASC_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ASC_dataset_2 = train_test_split[\"train\"].map(\n",
    "    createASC_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ABSA_dataset_1 = train_test_split[\"train\"].map(\n",
    "    createABSA_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ABSA_dataset_2 = train_test_split[\"train\"].map(\n",
    "    createABSA_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "ABSA_test = train_test_split[\"test\"].map(\n",
    "    createABSA_dataset, fn_kwargs={\"prompts_file_path\": \"prompts.json\"}\n",
    ")\n",
    "combined_datasets = concatenate_datasets(\n",
    "    [ATE_dataset_1, ASC_dataset_1, ABSA_dataset_1,ATE_dataset_2, ASC_dataset_2, ABSA_dataset_2 ],\n",
    ")\n",
    "combined_datasets = combined_datasets.shuffle()\n",
    "dataset_train_test = combined_datasets.train_test_split(test_size=0.1)\n",
    "\n",
    "final_ds = DatasetDict(\n",
    "    {\n",
    "        \"train\": dataset_train_test[\"train\"],\n",
    "        \"test\": dataset_train_test[\"test\"],\n",
    "        \"val\": ABSA_test,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:48:59.170768Z",
     "iopub.status.busy": "2024-05-25T17:48:59.170105Z",
     "iopub.status.idle": "2024-05-25T17:49:00.327955Z",
     "shell.execute_reply": "2024-05-25T17:49:00.327190Z",
     "shell.execute_reply.started": "2024-05-25T17:48:59.170747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map:   0%|          | 0/1809 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 1809/1809 [00:00<00:00, 4434.96 examples/s]\n",
      "Map: 100%|██████████| 1809/1809 [00:00<00:00, 39646.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/FRED-T5-large\", eos_token='</s>')\n",
    "tokenized_inputs = concatenate_datasets(\n",
    "    [\n",
    "        final_ds[\"train\"],\n",
    "        final_ds[\"test\"],\n",
    "        final_ds[\"val\"],\n",
    "    ],\n",
    ").map(\n",
    "    lambda x: tokenizer(x[\"aspect_ner_input\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"text\",\n",
    "        \"entities\",\n",
    "        \"aspect_list\",\n",
    "        \"ner_list\",\n",
    "        \"aspect_ner_list\",\n",
    "        \"aspect_ner_output\",\n",
    "        \"aspect_ner_input\",\n",
    "        \"__index_level_0__\",\n",
    "    ],\n",
    ")\n",
    "max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n",
    "\n",
    "tokenized_targets = concatenate_datasets(\n",
    "    [\n",
    "        final_ds[\"train\"],\n",
    "        final_ds[\"test\"],\n",
    "        final_ds[\"val\"],\n",
    "    ],\n",
    ").map(\n",
    "    lambda x: tokenizer(x[\"aspect_ner_output\"], truncation=True),\n",
    "    batched=True,\n",
    "    remove_columns=[\n",
    "        \"text\",\n",
    "        \"entities\",\n",
    "        \"aspect_list\",\n",
    "        \"ner_list\",\n",
    "        \"aspect_ner_list\",\n",
    "        \"aspect_ner_output\",\n",
    "        \"aspect_ner_input\",\n",
    "        \"__index_level_0__\",\n",
    "    ],\n",
    ")\n",
    "max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:49:00.329106Z",
     "iopub.status.busy": "2024-05-25T17:49:00.328739Z",
     "iopub.status.idle": "2024-05-25T17:49:00.359464Z",
     "shell.execute_reply": "2024-05-25T17:49:00.358889Z",
     "shell.execute_reply.started": "2024-05-25T17:49:00.329086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(\n",
    "    sample, tokenizer, max_source_length, max_target_length, padding=\"max_length\"\n",
    "):\n",
    "    \"\"\"function to tokenize data for FLAN\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        sample[\"aspect_ner_input\"],\n",
    "        max_length=max_source_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Tokenize targets with the `text_target` keyword argument\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"aspect_ner_output\"],\n",
    "        max_length=max_target_length,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # If we are padding here, replace all tokenizer.pad_token_id in the labels\n",
    "    # by -100 when we want to ignore\n",
    "    # padding in the loss.\n",
    "    if padding == \"max_length\":\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "            for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:49:00.360330Z",
     "iopub.status.busy": "2024-05-25T17:49:00.360081Z",
     "iopub.status.idle": "2024-05-25T17:49:01.189384Z",
     "shell.execute_reply": "2024-05-25T17:49:01.188710Z",
     "shell.execute_reply.started": "2024-05-25T17:49:00.360313Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1598/1598 [00:00<00:00, 2626.48 examples/s]\n",
      "Map: 100%|██████████| 178/178 [00:00<00:00, 2567.42 examples/s]\n",
      "Map: 100%|██████████| 33/33 [00:00<00:00, 1873.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = final_ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    fn_kwargs={\n",
    "        \"max_source_length\": max_source_length,\n",
    "        \"max_target_length\": max_target_length,\n",
    "        \"tokenizer\": tokenizer,\n",
    "    },\n",
    "    remove_columns=[\n",
    "        \"text\",\n",
    "        \"entities\",\n",
    "        \"aspect_list\",\n",
    "        \"ner_list\",\n",
    "        \"aspect_ner_list\",\n",
    "        \"aspect_ner_output\",\n",
    "        \"aspect_ner_input\",\n",
    "        \"__index_level_0__\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:49:01.190406Z",
     "iopub.status.busy": "2024-05-25T17:49:01.190150Z",
     "iopub.status.idle": "2024-05-25T17:49:18.034522Z",
     "shell.execute_reply": "2024-05-25T17:49:18.033769Z",
     "shell.execute_reply.started": "2024-05-25T17:49:01.190388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai-forever/FRED-T5-large\")\n",
    "    # we want to ignore tokenizer pad token in the loss\n",
    "label_pad_token_id = -100\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T17:49:18.037272Z",
     "iopub.status.busy": "2024-05-25T17:49:18.036425Z",
     "iopub.status.idle": "2024-05-25T18:26:40.523949Z",
     "shell.execute_reply": "2024-05-25T18:26:40.523366Z",
     "shell.execute_reply.started": "2024-05-25T17:49:18.037249Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 200/2000 [03:19<27:45,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.51it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.67it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.51it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.23it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.23it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                  \n",
      " 10%|█         | 200/2000 [03:25<27:45,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19681577384471893, 'eval_runtime': 6.9257, 'eval_samples_per_second': 25.701, 'eval_steps_per_second': 3.321, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 400/2000 [06:43<24:40,  1.08it/s]  \n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.50it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.67it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.51it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.23it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.23it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                  \n",
      " 20%|██        | 400/2000 [06:50<24:40,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08956189453601837, 'eval_runtime': 6.9244, 'eval_samples_per_second': 25.706, 'eval_steps_per_second': 3.322, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 500/2000 [08:29<24:45,  1.01it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3963, 'grad_norm': 0.9926210045814514, 'learning_rate': 8.333333333333334e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 600/2000 [10:08<21:36,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.51it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.67it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.51it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.23it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.23it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.23it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                  \n",
      " 30%|███       | 600/2000 [10:15<21:36,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04632798582315445, 'eval_runtime': 6.9234, 'eval_samples_per_second': 25.71, 'eval_steps_per_second': 3.322, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 800/2000 [13:33<18:29,  1.08it/s]  \n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.50it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.68it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.52it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                  \n",
      " 40%|████      | 800/2000 [13:40<18:29,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.03354667127132416, 'eval_runtime': 6.9235, 'eval_samples_per_second': 25.71, 'eval_steps_per_second': 3.322, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1000/2000 [16:58<15:25,  1.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0326, 'grad_norm': 0.7232740521430969, 'learning_rate': 5.555555555555556e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.51it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.68it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.51it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      " 50%|█████     | 1000/2000 [17:05<15:25,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.028165750205516815, 'eval_runtime': 6.8963, 'eval_samples_per_second': 25.811, 'eval_steps_per_second': 3.335, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1200/2000 [20:22<12:19,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.51it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.68it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.52it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      " 60%|██████    | 1200/2000 [20:29<12:19,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.020645666867494583, 'eval_runtime': 6.9222, 'eval_samples_per_second': 25.714, 'eval_steps_per_second': 3.323, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1400/2000 [23:47<09:15,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.51it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.68it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.52it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.28it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      " 70%|███████   | 1400/2000 [23:54<09:15,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.015695836395025253, 'eval_runtime': 6.9213, 'eval_samples_per_second': 25.718, 'eval_steps_per_second': 3.323, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1500/2000 [25:33<08:17,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0129, 'grad_norm': 0.17571555078029633, 'learning_rate': 2.777777777777778e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1600/2000 [27:12<06:10,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.50it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.67it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.52it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.23it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      " 80%|████████  | 1600/2000 [27:19<06:10,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.014588749967515469, 'eval_runtime': 6.9243, 'eval_samples_per_second': 25.706, 'eval_steps_per_second': 3.322, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1800/2000 [30:36<03:05,  1.08it/s]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.55it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.59it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.97it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.68it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.52it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.28it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.23it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      " 90%|█████████ | 1800/2000 [30:43<03:05,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "                                               \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011799040250480175, 'eval_runtime': 6.9227, 'eval_samples_per_second': 25.713, 'eval_steps_per_second': 3.322, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [34:01<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0069, 'grad_norm': 0.8691480755805969, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▊         | 2/23 [00:00<00:03,  6.52it/s]\u001b[A\n",
      " 13%|█▎        | 3/23 [00:00<00:04,  4.58it/s]\u001b[A\n",
      " 17%|█▋        | 4/23 [00:00<00:04,  3.96it/s]\u001b[A\n",
      " 22%|██▏       | 5/23 [00:01<00:04,  3.67it/s]\u001b[A\n",
      " 26%|██▌       | 6/23 [00:01<00:04,  3.51it/s]\u001b[A\n",
      " 30%|███       | 7/23 [00:01<00:04,  3.42it/s]\u001b[A\n",
      " 35%|███▍      | 8/23 [00:02<00:04,  3.36it/s]\u001b[A\n",
      " 39%|███▉      | 9/23 [00:02<00:04,  3.32it/s]\u001b[A\n",
      " 43%|████▎     | 10/23 [00:02<00:03,  3.29it/s]\u001b[A\n",
      " 48%|████▊     | 11/23 [00:03<00:03,  3.27it/s]\u001b[A\n",
      " 52%|█████▏    | 12/23 [00:03<00:03,  3.26it/s]\u001b[A\n",
      " 57%|█████▋    | 13/23 [00:03<00:03,  3.25it/s]\u001b[A\n",
      " 61%|██████    | 14/23 [00:04<00:02,  3.25it/s]\u001b[A\n",
      " 65%|██████▌   | 15/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 70%|██████▉   | 16/23 [00:04<00:02,  3.24it/s]\u001b[A\n",
      " 74%|███████▍  | 17/23 [00:04<00:01,  3.24it/s]\u001b[A\n",
      " 78%|███████▊  | 18/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 83%|████████▎ | 19/23 [00:05<00:01,  3.24it/s]\u001b[A\n",
      " 87%|████████▋ | 20/23 [00:05<00:00,  3.24it/s]\u001b[A\n",
      " 91%|█████████▏| 21/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      " 96%|█████████▌| 22/23 [00:06<00:00,  3.24it/s]\u001b[A\n",
      "                                                   \n",
      "100%|██████████| 2000/2000 [34:08<00:00,  1.08it/s]\n",
      "100%|██████████| 23/23 [00:06<00:00,  3.75it/s]\u001b[A\n",
      "100%|██████████| 2000/2000 [34:08<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.010190648958086967, 'eval_runtime': 6.8984, 'eval_samples_per_second': 25.803, 'eval_steps_per_second': 3.334, 'epoch': 10.0}\n",
      "{'train_runtime': 2048.8741, 'train_samples_per_second': 7.799, 'train_steps_per_second': 0.976, 'train_loss': 0.11216087079048156, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.11216087079048156, metrics={'train_runtime': 2048.8741, 'train_samples_per_second': 7.799, 'train_steps_per_second': 0.976, 'total_flos': 2.713057896824832e+16, 'train_loss': 0.11216087079048156, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_id = f\"fred-test\"\n",
    "\n",
    "    # Define training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=repository_id,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        predict_with_generate=True,\n",
    "        fp16=False,  # Overflows with fp16\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=10,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        optim=\"adamw_torch\",\n",
    "        # logging & evaluation strategies\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        save_total_limit=1,\n",
    "        push_to_hub=False,\n",
    "\n",
    "    )\n",
    "    # Create Trainer instance\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "    )\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T18:36:18.682870Z",
     "iopub.status.busy": "2024-05-25T18:36:18.682468Z",
     "iopub.status.idle": "2024-05-25T18:36:18.697518Z",
     "shell.execute_reply": "2024-05-25T18:36:18.696831Z",
     "shell.execute_reply.started": "2024-05-25T18:36:18.682849Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def generate_response(\n",
    "    model, tokenizer, question, top_p, temperature, prompts_path, device\n",
    "):\n",
    "    \"\"\"function to generate response from FLAN models\"\"\"\n",
    "    with open(prompts_path, encoding = 'UTF-8') as fp:\n",
    "        template = json.load(fp)\n",
    "    num = random.randint(1, len(template))\n",
    "    instruction =template[\"ABSA\"][str(num)]\n",
    "    input = f\"<LM>Задача: Извлечение и классификация именованных сущностей \\n{instruction}\\n{question}\\n\"\n",
    "    input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n",
    "    sample_output = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        do_sample=True,\n",
    "        max_length=100,\n",
    "        top_p=top_p,\n",
    "        temperature=temperature,\n",
    "        top_k=70,\n",
    "        early_stopping=True,\n",
    "        #no_repeat_ngram_size=2, \n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "\n",
    "    )\n",
    "    out = tokenizer.decode(sample_output[0][1:], skip_special_tokens=True)\n",
    "    if \"</s>\" in out:\n",
    "        out = out[: out.find(\"</s>\")].strip()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T18:39:18.586133Z",
     "iopub.status.busy": "2024-05-25T18:39:18.585717Z",
     "iopub.status.idle": "2024-05-25T18:39:18.607794Z",
     "shell.execute_reply": "2024-05-25T18:39:18.607061Z",
     "shell.execute_reply.started": "2024-05-25T18:39:18.586113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    trial_dataset,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    prompts_path,\n",
    "    top_p=0.5,\n",
    "    temperature=0.5,\n",
    "):\n",
    "    \"\"\"function to evaluate FLAN generation results\"\"\"\n",
    "\n",
    "    TP_aspect = 0\n",
    "    FN_aspect = 0\n",
    "    FP_aspect = 0\n",
    "    TP_sent = 0\n",
    "    FN_sent = 0\n",
    "    FP_sent = 0\n",
    "    answers = pd.DataFrame()\n",
    "\n",
    "    for i in trial_dataset:\n",
    "        answer = generate_response(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            question=i[\"text\"],\n",
    "            top_p=0.6,\n",
    "            temperature=0.7,\n",
    "            prompts_path=prompts_path,\n",
    "            device=device,\n",
    "        )\n",
    "        answer = answer.split(\"Ответ:\")[1].strip().replace(\": \", \":\")\n",
    "        new_row = {\"y_pred\": answer, \"y_true\": i[\"aspect_ner_list\"]}\n",
    "        answers = pd.concat([answers, pd.DataFrame([new_row])])\n",
    "\n",
    "        y_pred = answer.split(\",\")\n",
    "        y_true = i[\"aspect_ner_list\"].split(\",\")\n",
    "\n",
    "        aspects_true_lst = [item.split(\":\")[0] for item in y_true]\n",
    "        aspects_pred_lst = [item.split(\":\")[0] for item in y_pred]\n",
    "\n",
    "        for aspect in aspects_true_lst:\n",
    "            if aspect in aspects_pred_lst:\n",
    "                TP_aspect += 1\n",
    "            else:\n",
    "                FN_aspect += 1\n",
    "        for aspect in aspects_pred_lst:\n",
    "            if aspect not in aspects_true_lst:\n",
    "                FP_aspect += 1\n",
    "                FP_sent += 1\n",
    "\n",
    "        for item in y_true:\n",
    "            if item in y_pred:\n",
    "                TP_sent += 1\n",
    "            else:\n",
    "                FN_sent += 1\n",
    "\n",
    "    F1_aspect = 2 * TP_aspect / (2 * TP_aspect + FN_aspect + FP_aspect)\n",
    "    F1_sent = 2 * TP_sent / (2 * TP_sent + FN_sent + FP_sent)\n",
    "    F1_macro = (F1_aspect + F1_sent) / 2\n",
    "    F1_micro = (\n",
    "        2\n",
    "        * (TP_aspect + TP_sent)\n",
    "        / ((2 * (TP_aspect + TP_sent)) + (FN_aspect + FN_sent + FP_aspect + FP_sent))\n",
    "    )\n",
    "\n",
    "    print(F1_sent)\n",
    "    display(answers)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T18:39:23.782175Z",
     "iopub.status.busy": "2024-05-25T18:39:23.781791Z",
     "iopub.status.idle": "2024-05-25T18:39:59.890812Z",
     "shell.execute_reply": "2024-05-25T18:39:59.890094Z",
     "shell.execute_reply.started": "2024-05-25T18:39:23.782155Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614232209737828\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Табакошоп:ORG,Алкошоп:ORG,9:00:00:DATE,22:00:D...</td>\n",
       "      <td>potap64@npo.biz:MAIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KPR:TECH,+7 010 618 1753:TELEPHONE,HMN:ACRONYM...</td>\n",
       "      <td>KPR:ACRONYM,+7 010 618 1753:TELEPHONE,HMN:ACRO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока...</td>\n",
       "      <td>Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OguI1537:NUM,СК Согласие:ORG,Восточная горнору...</td>\n",
       "      <td>OguI1537:NUM,16.03.2006:DATE,СК Согласие:ORG,В...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovCha...</td>\n",
       "      <td>SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovCha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>028.36%:PERCENT,10.06.2000:DATE,28.12.2010:DAT...</td>\n",
       "      <td>28.36%:PERCENT,10.06.2000:DATE,10.06.2000:DATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,R...</td>\n",
       "      <td>Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM</td>\n",
       "      <td>WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH</td>\n",
       "      <td>MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM...</td>\n",
       "      <td>ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...</td>\n",
       "      <td>+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>СК Согласие:ORG,lk.x5.ru:MAIL,https://ip.org/:...</td>\n",
       "      <td>APL:TECH,СК Согласие:ORG,https://ip.org/:LINK,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OTX:TECH,JHX:TECH,https://ao.net/:LINK</td>\n",
       "      <td>OTX:TECH,JHX:TECH,https://ao.net/:LINK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>296.7%:PERCENT,286.7%:PERCENT,8:00:DATE,8:30:D...</td>\n",
       "      <td>96.7%:PERCENT,96.7%:PERCENT,96.7%:PERCENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+7 010 618 1753:TELEPHONE,Группа О'кей:ORG</td>\n",
       "      <td>+7 010 618 1753:TELEPHONE,Группа О'кей:ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...</td>\n",
       "      <td>+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Горячий Хлеб:ORG,Богданов Кирилл Ануфриевич:NA...</td>\n",
       "      <td>Богданов Кирилл Ануфриевич:NAME_EMPLOYEE,kozlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:...</td>\n",
       "      <td>GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH</td>\n",
       "      <td>UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT...</td>\n",
       "      <td>21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ФГ Будущее:ORG,ФГ Будущее:ORG,Таджикистан:ORG,...</td>\n",
       "      <td>ФГ Будущее:ORG,ФГ Будущее:ORG,ФГ Будущее:ORG,Ф...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FNG:ACRONYM,FNG:ACRONYM,А7:ACRONYM</td>\n",
       "      <td>FNG:ACRONYM,FNG:ACRONYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.23:NUM,HMN:ACRONYM,+7 010 618 1753:TELEPHONE...</td>\n",
       "      <td>+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SKC:TECH,15.10.2019:DATE,SKC:TECH,СК Согласие:ORG</td>\n",
       "      <td>SKC:TECH,15.10.2019:DATE,SKC:TECH,SKC:TECH,СК ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SKC:TECH</td>\n",
       "      <td>SKC:TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KvantumKore:TECH,IJFSystems:TECH,WYK:ACRONYM</td>\n",
       "      <td>KvantumKore:TECH,IJFSystems:TECH,WYK:TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RMV:ACRONYM,04.07.2003:DATE,hkAn1012:TECH,RMV:...</td>\n",
       "      <td>RMV:ACRONYM,04.07.2003:DATE,hkAn1012:NUM,TBQ:A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KosmoCode:TECH,DostoevskyData:TECH,TurgenevTec...</td>\n",
       "      <td>KosmoCode:TECH,DostoevskyData:TECH,TurgenevTec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PLU 4388:NUM,MTT:ACRONYM,EOX:ACRONYM</td>\n",
       "      <td>4388:NUM,MTT:ACRONYM,EOX:ACRONYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.20...</td>\n",
       "      <td>СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM</td>\n",
       "      <td>21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM</td>\n",
       "      <td>StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+7 010 618 1753:TELEPHONE,HMN:ACRONYM,PLU:ACRONYM</td>\n",
       "      <td>+7 010 618 1753:TELEPHONE,HMN:ACRONYM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              y_pred                                             y_true\n",
       "0  Табакошоп:ORG,Алкошоп:ORG,9:00:00:DATE,22:00:D...                               potap64@npo.biz:MAIL\n",
       "0  KPR:TECH,+7 010 618 1753:TELEPHONE,HMN:ACRONYM...  KPR:ACRONYM,+7 010 618 1753:TELEPHONE,HMN:ACRO...\n",
       "0  Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока...  Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока...\n",
       "0  OguI1537:NUM,СК Согласие:ORG,Восточная горнору...  OguI1537:NUM,16.03.2006:DATE,СК Согласие:ORG,В...\n",
       "0  SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovCha...  SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovCha...\n",
       "0  028.36%:PERCENT,10.06.2000:DATE,28.12.2010:DAT...  28.36%:PERCENT,10.06.2000:DATE,10.06.2000:DATE...\n",
       "0  Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,R...  Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,R...\n",
       "0       WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM       WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM\n",
       "0          MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH          MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH\n",
       "0  ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM...  ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM...\n",
       "0  +7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...  +7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...\n",
       "0  СК Согласие:ORG,lk.x5.ru:MAIL,https://ip.org/:...  APL:TECH,СК Согласие:ORG,https://ip.org/:LINK,...\n",
       "0             OTX:TECH,JHX:TECH,https://ao.net/:LINK             OTX:TECH,JHX:TECH,https://ao.net/:LINK\n",
       "0  296.7%:PERCENT,286.7%:PERCENT,8:00:DATE,8:30:D...          96.7%:PERCENT,96.7%:PERCENT,96.7%:PERCENT\n",
       "0         +7 010 618 1753:TELEPHONE,Группа О'кей:ORG         +7 010 618 1753:TELEPHONE,Группа О'кей:ORG\n",
       "0  +7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...  +7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...\n",
       "0  Горячий Хлеб:ORG,Богданов Кирилл Ануфриевич:NA...  Богданов Кирилл Ануфриевич:NAME_EMPLOYEE,kozlo...\n",
       "0  GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:...  GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:...\n",
       "0       UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH       UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH\n",
       "0  21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT...  21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT...\n",
       "0  ФГ Будущее:ORG,ФГ Будущее:ORG,Таджикистан:ORG,...  ФГ Будущее:ORG,ФГ Будущее:ORG,ФГ Будущее:ORG,Ф...\n",
       "0                 FNG:ACRONYM,FNG:ACRONYM,А7:ACRONYM                            FNG:ACRONYM,FNG:ACRONYM\n",
       "0  8.23:NUM,HMN:ACRONYM,+7 010 618 1753:TELEPHONE...  +7 010 618 1753:TELEPHONE,+7 010 618 1753:TELE...\n",
       "0  SKC:TECH,15.10.2019:DATE,SKC:TECH,СК Согласие:ORG  SKC:TECH,15.10.2019:DATE,SKC:TECH,SKC:TECH,СК ...\n",
       "0                                           SKC:TECH                                           SKC:TECH\n",
       "0       KvantumKore:TECH,IJFSystems:TECH,WYK:ACRONYM          KvantumKore:TECH,IJFSystems:TECH,WYK:TECH\n",
       "0  RMV:ACRONYM,04.07.2003:DATE,hkAn1012:TECH,RMV:...  RMV:ACRONYM,04.07.2003:DATE,hkAn1012:NUM,TBQ:A...\n",
       "0  KosmoCode:TECH,DostoevskyData:TECH,TurgenevTec...  KosmoCode:TECH,DostoevskyData:TECH,TurgenevTec...\n",
       "0               PLU 4388:NUM,MTT:ACRONYM,EOX:ACRONYM                   4388:NUM,MTT:ACRONYM,EOX:ACRONYM\n",
       "0  СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.20...  СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.20...\n",
       "0            21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM            21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM\n",
       "0         StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM         StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM\n",
       "0  +7 010 618 1753:TELEPHONE,HMN:ACRONYM,PLU:ACRONYM              +7 010 618 1753:TELEPHONE,HMN:ACRONYM"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answers = evaluate(\n",
    "        trial_dataset=final_ds[\"val\"],\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=device,\n",
    "        prompts_path=\"prompts.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T18:40:21.743615Z",
     "iopub.status.busy": "2024-05-25T18:40:21.743224Z",
     "iopub.status.idle": "2024-05-25T18:40:21.778146Z",
     "shell.execute_reply": "2024-05-25T18:40:21.777410Z",
     "shell.execute_reply.started": "2024-05-25T18:40:21.743594Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Табакошоп:ORG,Алкошоп:ORG,9:00:00:DATE,22:00:DATE:TELEPHONE,potap64@npo.biz:MAIL\n",
      "potap64@npo.biz:MAIL\n",
      "===================\n",
      "KPR:TECH,+7 010 618 1753:TELEPHONE,HMN:ACRONYM,PLU:TECH,5800:NUM,6016:NUM,1612:NUM,3805:NUM,8512:NUM,9603:NUM,9332:NUM,QGN:ACRONYM\n",
      "KPR:ACRONYM,+7 010 618 1753:TELEPHONE,HMN:ACRONYM,5800:NUM,6016:NUM,989:NUM,1612:NUM,3805:NUM,8512:NUM,637:NUM,9603:NUM,9332:NUM,QGN:ACRONYM\n",
      "===================\n",
      "Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG\n",
      "Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG,Кока-Кола Эйчбиси Евразия (Coca-Cola):ORG\n",
      "===================\n",
      "OguI1537:NUM,СК Согласие:ORG,Восточная горнорудная компания:ORG\n",
      "OguI1537:NUM,16.03.2006:DATE,СК Согласие:ORG,Восточная горнорудная компания:ORG\n",
      "===================\n",
      "SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovChain:TECH\n",
      "SFB:TECH,ChekhovChain:TECH,SFB:TECH,ChekhovChain:TECH\n",
      "===================\n",
      "028.36%:PERCENT,10.06.2000:DATE,28.12.2010:DATE,6.24%:PERCENT\n",
      "28.36%:PERCENT,10.06.2000:DATE,10.06.2000:DATE,6.24%:PERCENT\n",
      "===================\n",
      "Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,RKT:ORG\n",
      "Railgo:ORG,Трест КХМ:ORG,Концерн Титан-2:ORG,RKT:ACRONYM\n",
      "===================\n",
      "WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM\n",
      "WRL:ACRONYM,CYU:ACRONYM,OZP:TECH,RMV:ACRONYM\n",
      "===================\n",
      "MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH\n",
      "MDW:ACRONYM,YGL:TECH,YTM:ACRONYM,YGL:TECH\n",
      "===================\n",
      "ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM,+7 010 618 1753:TELEPHONE,СК Согласие:ORG,СК Согласие:ORG\n",
      "ZMV:TECH,+7 010 618 1753:TELEPHONE,RMV:ACRONYM,RMV:ACRONYM,+7 010 618 1753:TELEPHONE,СК Согласие:ORG,СК Согласие:ORG\n",
      "===================\n",
      "+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE\n",
      "+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE\n",
      "===================\n",
      "СК Согласие:ORG,lk.x5.ru:MAIL,https://ip.org/:LINK,KosmoCode:TECH\n",
      "APL:TECH,СК Согласие:ORG,https://ip.org/:LINK,KosmoCode:TECH\n",
      "===================\n",
      "OTX:TECH,JHX:TECH,https://ao.net/:LINK\n",
      "OTX:TECH,JHX:TECH,https://ao.net/:LINK\n",
      "===================\n",
      "296.7%:PERCENT,286.7%:PERCENT,8:00:DATE,8:30:DATE,296.7%:PERCENT\n",
      "96.7%:PERCENT,96.7%:PERCENT,96.7%:PERCENT\n",
      "===================\n",
      "+7 010 618 1753:TELEPHONE,Группа О'кей:ORG\n",
      "+7 010 618 1753:TELEPHONE,Группа О'кей:ORG\n",
      "===================\n",
      "+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE,IJFSystems:TECH\n",
      "+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE,IJFSystems:TECH\n",
      "===================\n",
      "Горячий Хлеб:ORG,Богданов Кирилл Ануфриевич:NAME,kozlovruben@obedinennaja.org:MAIL,HNV:ACRONYM,HNV:ACRONYM\n",
      "Богданов Кирилл Ануфриевич:NAME_EMPLOYEE,kozlovruben@obedinennaja.org:MAIL,HNV:ACRONYM,HNV:ACRONYM\n",
      "===================\n",
      "GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:TECH,WYK:ACRONYM\n",
      "GogolGadget:TECH,SKC:TECH,OZP:TECH,IJFSystems:TECH,WYK:TECH\n",
      "===================\n",
      "UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH\n",
      "UMC:TECH,PLQ:TECH,RQN:TECH,PLQ:TECH,UMC:TECH\n",
      "===================\n",
      "21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT,22.06%:PERCENT,ZSQ:ACRONYM\n",
      "21.06.2016:DATE,14.11.2004:DATE,22.06%:PERCENT,22.06%:PERCENT,29.04.2003:DATE,ZSQ:ACRONYM\n",
      "===================\n",
      "ФГ Будущее:ORG,ФГ Будущее:ORG,Таджикистан:ORG,Узбекистан:ORG,ФГ Будущее:ORG\n",
      "ФГ Будущее:ORG,ФГ Будущее:ORG,ФГ Будущее:ORG,ФГ Будущее:ORG,ФГ Будущее:ORG\n",
      "===================\n",
      "FNG:ACRONYM,FNG:ACRONYM,А7:ACRONYM\n",
      "FNG:ACRONYM,FNG:ACRONYM\n",
      "===================\n",
      "8.23:NUM,HMN:ACRONYM,+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE,HMN:ACRONYM,28.36%:PERCENT\n",
      "+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE,+7 010 618 1753:TELEPHONE,HMN:ACRONYM,28.36%:PERCENT\n",
      "===================\n",
      "SKC:TECH,15.10.2019:DATE,SKC:TECH,СК Согласие:ORG\n",
      "SKC:TECH,15.10.2019:DATE,SKC:TECH,SKC:TECH,СК Согласие:ORG\n",
      "===================\n",
      "SKC:TECH\n",
      "SKC:TECH\n",
      "===================\n",
      "KvantumKore:TECH,IJFSystems:TECH,WYK:ACRONYM\n",
      "KvantumKore:TECH,IJFSystems:TECH,WYK:TECH\n",
      "===================\n",
      "RMV:ACRONYM,04.07.2003:DATE,hkAn1012:TECH,RMV:ACRONYM,TBQ:ACRONYM,RMV:ACRONYM\n",
      "RMV:ACRONYM,04.07.2003:DATE,hkAn1012:NUM,TBQ:ACRONYM,RMV:ACRONYM\n",
      "===================\n",
      "KosmoCode:TECH,DostoevskyData:TECH,TurgenevTech:TECH,EYR:TECH,ivan.ivanov:TECH,NWN:ACRONYM,http://rao.biz/:LINK\n",
      "KosmoCode:TECH,DostoevskyData:TECH,TurgenevTech:TECH,EYR:TECH,NWN:ACRONYM,http://rao.biz/:LINK\n",
      "===================\n",
      "PLU 4388:NUM,MTT:ACRONYM,EOX:ACRONYM\n",
      "4388:NUM,MTT:ACRONYM,EOX:ACRONYM\n",
      "===================\n",
      "СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.2016:DATE\n",
      "СК Согласие:ORG,ГК Агро-Белогорье:ORG,13.07.2016:DATE\n",
      "===================\n",
      "21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM\n",
      "21.10.2015:DATE,IZG:ACRONYM,ZSQ:ACRONYM\n",
      "===================\n",
      "StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM\n",
      "StrelkaStream:TECH,TSJ:ACRONYM,TSJ:ACRONYM\n",
      "===================\n",
      "+7 010 618 1753:TELEPHONE,HMN:ACRONYM,PLU:ACRONYM\n",
      "+7 010 618 1753:TELEPHONE,HMN:ACRONYM\n",
      "===================\n"
     ]
    }
   ],
   "source": [
    "predictions = list(answers['y_pred'])\n",
    "labels = list(answers['y_true'])\n",
    "\n",
    "for pred, label in zip(predictions, labels):\n",
    "    print(pred)\n",
    "    print(label)\n",
    "    print('===================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T18:40:35.986695Z",
     "iopub.status.busy": "2024-05-25T18:40:35.986303Z",
     "iopub.status.idle": "2024-05-25T18:43:02.994674Z",
     "shell.execute_reply": "2024-05-25T18:43:02.994070Z",
     "shell.execute_reply.started": "2024-05-25T18:40:35.986674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/fred/tokenizer_config.json',\n",
       " './results/fred/special_tokens_map.json',\n",
       " './results/fred/vocab.json',\n",
       " './results/fred/merges.txt',\n",
       " './results/fred/added_tokens.json',\n",
       " './results/fred/tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./results/fred\")\n",
    "tokenizer.save_pretrained(\"./results/fred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
