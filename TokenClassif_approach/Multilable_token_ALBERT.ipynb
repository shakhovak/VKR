{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABSA как многомерная классификация токенов 2\n",
    "\n",
    "Данный ноутбук повторяет код и з ноутбука с Bert, только изменена трансофрмерная модель на Albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea4cc5e",
   "metadata": {
    "cellId": "jf08le6wanhck1c6dz0zb9"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "138bf4ac",
   "metadata": {
    "cellId": "bok68tvdnkwbyup9ig60d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b3915b3eb843b1ae8f4134b9d43221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/10.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83523436146e4743a49e8aecb276a3f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f3c3c061404e4eac5f6feaa752194b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d2b5699406415d851b4e0719564a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44332b97d886428384aa6fd19d684ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/359k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a594e3a3749435eaef2baaa600224be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e49b67cb3ed4e8da491931d75ca86db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating trial split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba58fd71bfe04f35a48ddbcd017e822d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9e4c9b95b44de1bb9413582aecb9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"alexcadillon/SemEval2014Task4\", 'restaurants')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa1bf184",
   "metadata": {
    "cellId": "pyhnpxgb4mf65d2izo5nxd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3359',\n",
       " 'text': 'The pizza is the best if you like thin crusted pizza.',\n",
       " 'aspectTerms': [{'term': 'pizza',\n",
       "   'polarity': 'positive',\n",
       "   'from': '4',\n",
       "   'to': '9'},\n",
       "  {'term': 'thin crusted pizza',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '34',\n",
       "   'to': '52'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "sample = raw_datasets['train'][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3576a62",
   "metadata": {
    "cellId": "use8tx7st6u3ptbwo6wzq"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "label_list = ['O', 'B', 'I', 'E', 'S', 'POS', 'NEG', 'NEU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0f6368",
   "metadata": {
    "cellId": "jqpnkhq5qw47fm7li59m2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4, 'POS': 5, 'NEG': 6, 'NEU': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "id2label={i: l for i, l in enumerate(label_list)}\n",
    "label2id={l: i for i, l in enumerate(label_list)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b621c7d1",
   "metadata": {
    "cellId": "o0etkkg9v300o7ucjsblfk"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(example):\n",
    "    terms = []\n",
    "    polarities = []\n",
    "    for i in example['aspectTerms']:\n",
    "        terms.append(i['term'])\n",
    "        polarities.append(i['polarity'])\n",
    "\n",
    "    ner_tag = []\n",
    "    tokens = []\n",
    "    for term in terms:\n",
    "        if len(term.split(' '))> 1:\n",
    "            if polarities[terms.index(term)] == 'positive':\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,5])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,5])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,5])\n",
    "                        tokens.append(tok)\n",
    "            elif polarities[terms.index(term)] == 'negative':\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,6])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,6])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,6])\n",
    "                        tokens.append(tok)\n",
    "            else:\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,7])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,7])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,7])\n",
    "                        tokens.append(tok)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if polarities[terms.index(term)] == 'positive':\n",
    "                ner_tag.append([4,5])\n",
    "                tokens.append(term)\n",
    "            elif polarities[terms.index(term)] == 'negative':\n",
    "                ner_tag.append([4,6])\n",
    "                tokens.append(term)\n",
    "            else:\n",
    "                ner_tag.append([4,7])\n",
    "                tokens.append(term)\n",
    "\n",
    "    ner_tag_list_fin = []\n",
    "    tokens_fin = []\n",
    "    for token in word_tokenize(example['text']):\n",
    "        if token in tokens:\n",
    "            ner_tag_list_fin.append(ner_tag[tokens.index(token)])\n",
    "            tokens_fin.append(token)\n",
    "            ner_tag.pop(tokens.index(token))\n",
    "            tokens.pop(tokens.index(token))\n",
    "        else:\n",
    "            ner_tag_list_fin.append([0,0])\n",
    "            tokens_fin.append(token)\n",
    "\n",
    "    example['ner_tag'] = ner_tag_list_fin\n",
    "    example['tokens'] = tokens_fin\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddbec7a5",
   "metadata": {
    "cellId": "rla1zodjxltg4mu2ls641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3359',\n",
       " 'text': 'The pizza is the best if you like thin crusted pizza.',\n",
       " 'aspectTerms': [{'term': 'pizza',\n",
       "   'polarity': 'positive',\n",
       "   'from': '4',\n",
       "   'to': '9'},\n",
       "  {'term': 'thin crusted pizza',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '34',\n",
       "   'to': '52'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}],\n",
       " 'ner_tag': [[0, 0],\n",
       "  [4, 5],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [1, 7],\n",
       "  [2, 7],\n",
       "  [3, 7],\n",
       "  [0, 0]],\n",
       " 'tokens': ['The',\n",
       "  'pizza',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'if',\n",
       "  'you',\n",
       "  'like',\n",
       "  'thin',\n",
       "  'crusted',\n",
       "  'pizza',\n",
       "  '.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "preprocess_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa21ec0",
   "metadata": {
    "cellId": "g01q6j2zm9bgw6hw2s43de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_text at 0x7f5d405efeb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485d043c89c54dba824fcb33d619c1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a48f02ad19472ba65e5362753c0e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f8bf47438048b684b4c716ea599f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "dataset = raw_datasets.map(\n",
    "    preprocess_text,\n",
    "    remove_columns = ['text', 'aspectTerms', 'aspectCategories']\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8982c15",
   "metadata": {
    "cellId": "deb0guhtiy9l1pfyd38u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three___________________________________[0, 0]\n",
      "courses_________________________________[4, 7]\n",
      "-_______________________________________[0, 0]\n",
      "choices_________________________________[0, 0]\n",
      "include_________________________________[0, 0]\n",
      "excellent_______________________________[0, 0]\n",
      "mussels_________________________________[4, 5]\n",
      ",_______________________________________[0, 0]\n",
      "puff____________________________________[1, 5]\n",
      "pastry__________________________________[2, 5]\n",
      "goat____________________________________[2, 5]\n",
      "cheese__________________________________[3, 5]\n",
      "and_____________________________________[0, 0]\n",
      "salad___________________________________[1, 5]\n",
      "with____________________________________[2, 5]\n",
      "a_______________________________________[2, 5]\n",
      "delicious_______________________________[2, 5]\n",
      "dressing________________________________[3, 5]\n",
      ",_______________________________________[0, 0]\n",
      "and_____________________________________[0, 0]\n",
      "a_______________________________________[0, 0]\n",
      "hanger__________________________________[1, 5]\n",
      "steak___________________________________[2, 5]\n",
      "au______________________________________[2, 5]\n",
      "poivre__________________________________[3, 5]\n",
      "that____________________________________[0, 0]\n",
      "is______________________________________[0, 0]\n",
      "out_____________________________________[0, 0]\n",
      "of______________________________________[0, 0]\n",
      "this____________________________________[0, 0]\n",
      "world___________________________________[0, 0]\n",
      "._______________________________________[0, 0]\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['train'][50]['tokens'], dataset['train'][50]['ner_tag']):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdcdbab6",
   "metadata": {
    "cellId": "jg9oj0c9cicmzdpm0ilh8"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8a386f",
   "metadata": {
    "cellId": "d364s0hvvcsxxeky83rwr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8f9dfe47e8460789e38440fe84d5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/710 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fb53f48d7c4cd2aaf099bd16242603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7311e5ef5e694426a099b343ecfb220e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.1\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c1ae0a6",
   "metadata": {
    "cellId": "expakkar3ibvvwn6k0f5ws"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "label_count = len(label_list)\n",
    "def tokenize_and_align_labels(examples, label_all_tokens: bool = False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tag\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append([-100 for l in range(label_count)])\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append([1 if l in label[word_idx] else 0 for l in range(label_count)])\n",
    "            else:\n",
    "                label_ids.append([1 if l in label[word_idx] else 0 for l in range(label_count)]\n",
    "                                     if label_all_tokens else [-100 for l in range(label_count)])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9c20ad",
   "metadata": {
    "cellId": "fzr3senl4d2j3b1xuf86p"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9885387b231341e88950eae8f52d760a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3feecc89a4414d35a1d54bd019216b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35107f590ebb4ec1ae0297f952a65a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True,\n",
    "                               remove_columns = ['sentenceId', 'tokens', 'ner_tag'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "636b9e69",
   "metadata": {
    "cellId": "ii1wd6lxliiy9d3f9kuhs"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "tokenized_dataset.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6381584",
   "metadata": {
    "cellId": "04jyv1cmls2y2eqkpej64dr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁they___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁did____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁not____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁have___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁may____________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "on______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "na______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "ise_____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      ",_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁forgot_________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁our____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁toast__________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      ",_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁left___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁out____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁ingredients____________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "(_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "ie______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁cheese_________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "▁in_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁an_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁_______________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "ome_____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "let_____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      ")_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      ",_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁below__________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁hot____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁temperatures___________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁and____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁the____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁bacon__________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "▁was____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁so_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁over___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁cooked_________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁it_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁crumble________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "d_______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "▁on_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁the____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁plate__________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "▁when___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁you____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁touched________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁it_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "▁_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "._______________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "[SEP]___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(tokenized_dataset['train'][10]['input_ids']), \n",
    "                        tokenized_dataset['train'][10]['labels']):\n",
    "    print(f'{token:_<40}{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09fa34e7",
   "metadata": {
    "cellId": "llzzbwc83qzia46vjrj6q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:47:14.559684: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8ee2f0f",
   "metadata": {
    "cellId": "x5nq46rmc8dle6cp8h4qwq"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "921efdb2",
   "metadata": {
    "cellId": "cj3rmjbuum9r70qv20tit"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "class MultiLabelNERTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[torch.FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels  = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # this accesses predictions for tokens that aren't CLS, PAD, or the 2nd+ subword in a word\n",
    "        # and simultaneously flattens the logits or labels\n",
    "        flat_outputs = outputs.logits[labels!=-100] \n",
    "        flat_labels  = labels[ labels!=-100]\n",
    "        \n",
    "        try:\n",
    "            loss = self.loss_fct(flat_outputs, flat_labels.float())\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(flat_outputs, flat_labels.float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc54192d",
   "metadata": {
    "cellId": "hqeqgvh90ddsv9d786dpbi"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def compute_metrics(p, threshold=0.5):\n",
    "    predictions, labels = p\n",
    "\n",
    "    cleaned_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if [i for i in l if i != -100]]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "    true_predictions = []\n",
    "    for prediction in cleaned_predictions:\n",
    "        temp = sigmoid(torch.Tensor(prediction))\n",
    "        y_pred = np.zeros(temp.shape)\n",
    "        y_pred[np.where(temp >= 0.5)] = 1\n",
    "        true_predictions.extend(y_pred)\n",
    "    \n",
    "    cleaned_labels = [\n",
    "    [l for (p, l) in zip(prediction, label) if [i for i in l if i != -100]]\n",
    "    for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = []\n",
    "    for label in cleaned_labels:\n",
    "        temp = np.array(label)\n",
    "        true_labels.extend(temp)\n",
    "        \n",
    "    f1_micro_average = f1_score(y_true=true_labels, y_pred=true_predictions, average='macro')\n",
    "    roc_auc = roc_auc_score(true_labels, true_predictions, average = 'macro')\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    \n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddc2b034",
   "metadata": {
    "cellId": "r78bsvqrd1n2nttuhwgi33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e92f5c5111e4cbaaebd8252d7a8a1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/893M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15205' max='15205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15205/15205 21:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.046359</td>\n",
       "      <td>0.681261</td>\n",
       "      <td>0.815947</td>\n",
       "      <td>0.934955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.038907</td>\n",
       "      <td>0.782420</td>\n",
       "      <td>0.878151</td>\n",
       "      <td>0.952464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.032837</td>\n",
       "      <td>0.840113</td>\n",
       "      <td>0.897709</td>\n",
       "      <td>0.961971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.038225</td>\n",
       "      <td>0.837096</td>\n",
       "      <td>0.895187</td>\n",
       "      <td>0.963080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.037448</td>\n",
       "      <td>0.842651</td>\n",
       "      <td>0.900629</td>\n",
       "      <td>0.963397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15205, training_loss=0.0423736910175548, metrics={'train_runtime': 1272.5272, 'train_samples_per_second': 11.949, 'train_steps_per_second': 11.949, 'total_flos': 390501884377440.0, 'train_loss': 0.0423736910175548, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"albert-xxlarge-v2\", problem_type=\"multi_label_classification\",\n",
    "    num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"token_class_model\",\n",
    "    learning_rate=5e-06,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"no\", \n",
    "    group_by_length=True,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = MultiLabelNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1054dff6",
   "metadata": {
    "cellId": "tijwo1374uoq19pxor00r8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.1\n",
    "trial = trainer.predict(tokenized_dataset['trial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f05793a4",
   "metadata": {
    "cellId": "iyvnyxuagh7vzdgkgjq3f"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "preds = trial.predictions[0]\n",
    "label = trial.label_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88373bd9",
   "metadata": {
    "cellId": "4ya89apcn5ldcfddlmd4hi"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "def return_labels(preds, label):   \n",
    "    all_preds = []\n",
    "    for pred, lbl in zip(preds, label):\n",
    "        true_pred = [p for (p,l) in zip(pred, lbl) if l != -100]\n",
    "        if len(true_pred) > 0:\n",
    "            temp = sigmoid(torch.Tensor(true_pred))\n",
    "            y_pred = np.zeros(temp.shape)\n",
    "            y_pred[np.where(temp >= 0.5)] = 1\n",
    "            all_preds.append(y_pred.tolist())\n",
    "    \n",
    "    tags = []\n",
    "    for token in all_preds:\n",
    "        if sum(i for i in token)> 1:\n",
    "            temp = []\n",
    "            for indx, element in enumerate(token):\n",
    "                #temp = []\n",
    "                if element !=0:\n",
    "                    temp.append(id2label[indx])\n",
    "            tags.append(temp)\n",
    "        else:\n",
    "            for indx, element in enumerate(token):\n",
    "                if element !=0:\n",
    "                    tags.append(id2label[indx])\n",
    "    return tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1dd85ff",
   "metadata": {
    "cellId": "ttxeiumxmpaajt0ivsg4g7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '1609',\n",
       " 'text': 'Service was quick.',\n",
       " 'aspectTerms': [{'term': 'Service',\n",
       "   'polarity': 'positive',\n",
       "   'from': '0',\n",
       "   'to': '7'}],\n",
       " 'aspectCategories': [{'category': 'service', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbd06eeb",
   "metadata": {
    "cellId": "l8le19t53apv0zi4ppxp0g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service_________________________________['S', 'POS']\n",
      "was_____________________________________O\n",
      "quick___________________________________O\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][5]['tokens'], return_labels(trial.predictions[5], trial.label_ids[5])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b6df6ac",
   "metadata": {
    "cellId": "5unlebfo95ivxbvfba0sn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3041',\n",
       " 'text': \"We've only eaten in the restaurant once, but we have ordered many times for dinner.\",\n",
       " 'aspectTerms': [{'term': 'dinner',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '76',\n",
       "   'to': '82'}],\n",
       " 'aspectCategories': [{'category': 'anecdotes/miscellaneous',\n",
       "   'polarity': 'neutral'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd8ec443",
   "metadata": {
    "cellId": "65dayhndwu6c1aros8407v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We______________________________________O\n",
      "'ve_____________________________________O\n",
      "only____________________________________O\n",
      "eaten___________________________________O\n",
      "in______________________________________O\n",
      "the_____________________________________O\n",
      "restaurant______________________________O\n",
      "once____________________________________O\n",
      ",_______________________________________O\n",
      "but_____________________________________O\n",
      "we______________________________________O\n",
      "have____________________________________O\n",
      "ordered_________________________________O\n",
      "many____________________________________O\n",
      "times___________________________________O\n",
      "for_____________________________________O\n",
      "dinner__________________________________['S', 'NEU']\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][99]['tokens'], return_labels(trial.predictions[99], trial.label_ids[99])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2a436d",
   "metadata": {
    "cellId": "vnmijp6lc1wbc1qqoilu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3170',\n",
       " 'text': 'Good spreads, great beverage selections and bagels really tasty.',\n",
       " 'aspectTerms': [{'term': 'spreads',\n",
       "   'polarity': 'positive',\n",
       "   'from': '5',\n",
       "   'to': '12'},\n",
       "  {'term': 'beverage selections',\n",
       "   'polarity': 'positive',\n",
       "   'from': '20',\n",
       "   'to': '39'},\n",
       "  {'term': 'bagels', 'polarity': 'positive', 'from': '44', 'to': '50'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "edb1f4d1",
   "metadata": {
    "cellId": "e9ujs6xw94i6z0g1tybij6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good____________________________________O\n",
      "spreads_________________________________['S', 'POS']\n",
      ",_______________________________________O\n",
      "great___________________________________O\n",
      "beverage________________________________['B', 'POS']\n",
      "selections______________________________['E', 'POS']\n",
      "and_____________________________________O\n",
      "bagels__________________________________['S', 'POS']\n",
      "really__________________________________O\n",
      "tasty___________________________________O\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][40]['tokens'], return_labels(trial.predictions[40], trial.label_ids[40])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20b192ff",
   "metadata": {
    "cellId": "tz5t53rud9cl47s4a9u2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good____________________________________O [0, 0]\n",
      "spreads_________________________________['S', 'POS'] [4, 5]\n",
      ",_______________________________________O [0, 0]\n",
      "great___________________________________O [0, 0]\n",
      "beverage________________________________['B', 'POS'] [1, 5]\n",
      "selections______________________________['E', 'POS'] [3, 5]\n",
      "and_____________________________________O [0, 0]\n",
      "bagels__________________________________['S', 'POS'] [4, 5]\n",
      "really__________________________________O [0, 0]\n",
      "tasty___________________________________O [0, 0]\n",
      "._______________________________________O [0, 0]\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag, label in zip(dataset['trial'][40]['tokens'], return_labels(trial.predictions[40], trial.label_ids[40]),\\\n",
    "                                 dataset['trial'][40]['ner_tag']):\n",
    "    print(f'{token:_<40}{ner_tag} {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9074620",
   "metadata": {
    "cellId": "l5lpmg8awkfg4ajx5csa26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on E2E ABSA: 0.7568238213399504\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "trial = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(trial.predictions)):\n",
    "    pred = return_labels(trial.predictions[i], trial.label_ids[i])\n",
    "    predictions.append(pred)\n",
    "\n",
    "count_total = 0\n",
    "count_matches = 0\n",
    "for label, prediction in zip(dataset['test']['ner_tag'],predictions):\n",
    "    for lbl, pred in zip(label, prediction):\n",
    "        if lbl != [0,0]:\n",
    "            count_total += 1\n",
    "            new_label = []\n",
    "            for item in lbl:\n",
    "                \n",
    "                new_item = id2label[item]\n",
    "                new_label.append(new_item)\n",
    "            if new_label == pred:\n",
    "                count_matches += 1\n",
    "print(f'Accuracy on E2E ABSA: {count_matches/count_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальные выводы\n",
    "\n",
    "Интересно, что Albert сработал лучше, чем Bert с большим кол-вом параметров как и для задач классификации токенов, так и в извлечении аспекта и его тональности. В целом, эксперименты показали, что переход к многомреной классификации дает прирост точности и данный подход может быть использован в дальнейшем для доведения точности модели до уровня SOTA."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "4206581f-793b-4842-9b66-056519aa4620",
  "notebookPath": "Multilable_token_ALBERT.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
