{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи ABSA как классификации токена с несколькими лейблами\n",
    "\n",
    "В данной задаче лейбл для токена не соединяется в один тег, а предсатвляется в виде вектора нулей и единиц, где каждое число отвечает за определенный тег. Таким образом, лейбл может иметь несколько разных тегов, но не может иметь несколько одинаковых тегов. Теги выглядят следующим образом : ['O', 'B', 'I', 'E', 'S', 'POS', 'NEG', 'NEU']. Каждый элемент может быть выбран один раз. Это связано с особенностями функции потерь BCEWithLogitsLoss, которая может работать с нулями и единицами.\n",
    "\n",
    "По сравнению с предыдущей задачей, помимо логики присваивания тега, я также поменяла функцию потерь и оценки результатов классификации. В остальном - код такой же, как и при одномерной классификации токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9390b0bb",
   "metadata": {
    "cellId": "jf08le6wanhck1c6dz0zb9"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12149dde",
   "metadata": {
    "cellId": "bok68tvdnkwbyup9ig60d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bf1ba10bdf4f2c8f5569da27f2e7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/10.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d7de6f41f44bb589954cd9f4f72eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5489b109a8847b698744a7a02361959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fa3aebe1484236809e87a3e9c709a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2e95cf95e34cb2a959ff4732b0230f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/359k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1870886c2f45f9a4db0e2fc163a666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c0a414c28549f8be8c0dd51cfbd3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating trial split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3bbcbe173e471f96dd4747b626d42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd383ffbabd5441e9b32582703a17820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentenceId', 'text', 'aspectTerms', 'aspectCategories'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"alexcadillon/SemEval2014Task4\", 'restaurants')\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b906d7d",
   "metadata": {
    "cellId": "pyhnpxgb4mf65d2izo5nxd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3359',\n",
       " 'text': 'The pizza is the best if you like thin crusted pizza.',\n",
       " 'aspectTerms': [{'term': 'pizza',\n",
       "   'polarity': 'positive',\n",
       "   'from': '4',\n",
       "   'to': '9'},\n",
       "  {'term': 'thin crusted pizza',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '34',\n",
       "   'to': '52'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "sample = raw_datasets['train'][15]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923f2beb",
   "metadata": {
    "cellId": "use8tx7st6u3ptbwo6wzq"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "label_list = ['O', 'B', 'I', 'E', 'S', 'POS', 'NEG', 'NEU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd9fc05",
   "metadata": {
    "cellId": "jqpnkhq5qw47fm7li59m2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2, 'E': 3, 'S': 4, 'POS': 5, 'NEG': 6, 'NEU': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "id2label={i: l for i, l in enumerate(label_list)}\n",
    "label2id={l: i for i, l in enumerate(label_list)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46779b27",
   "metadata": {
    "cellId": "o0etkkg9v300o7ucjsblfk"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(example):\n",
    "    terms = []\n",
    "    polarities = []\n",
    "    for i in example['aspectTerms']:\n",
    "        terms.append(i['term'])\n",
    "        polarities.append(i['polarity'])\n",
    "\n",
    "    ner_tag = []\n",
    "    tokens = []\n",
    "    for term in terms:\n",
    "        if len(term.split(' '))> 1:\n",
    "            if polarities[terms.index(term)] == 'positive':\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,5])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,5])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,5])\n",
    "                        tokens.append(tok)\n",
    "            elif polarities[terms.index(term)] == 'negative':\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,6])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,6])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,6])\n",
    "                        tokens.append(tok)\n",
    "            else:\n",
    "                token_list = term.split(' ')\n",
    "                for ind, tok in enumerate(token_list):\n",
    "                    if ind == 0:\n",
    "                        ner_tag.append([1,7])\n",
    "                        tokens.append(tok)\n",
    "                    elif ind == len(token_list)-1:\n",
    "                        ner_tag.append([3,7])\n",
    "                        tokens.append(tok)\n",
    "                    else:\n",
    "                        ner_tag.append([2,7])\n",
    "                        tokens.append(tok)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if polarities[terms.index(term)] == 'positive':\n",
    "                ner_tag.append([4,5])\n",
    "                tokens.append(term)\n",
    "            elif polarities[terms.index(term)] == 'negative':\n",
    "                ner_tag.append([4,6])\n",
    "                tokens.append(term)\n",
    "            else:\n",
    "                ner_tag.append([4,7])\n",
    "                tokens.append(term)\n",
    "\n",
    "    ner_tag_list_fin = []\n",
    "    tokens_fin = []\n",
    "    for token in word_tokenize(example['text']):\n",
    "        if token in tokens:\n",
    "            ner_tag_list_fin.append(ner_tag[tokens.index(token)])\n",
    "            tokens_fin.append(token)\n",
    "            ner_tag.pop(tokens.index(token))\n",
    "            tokens.pop(tokens.index(token))\n",
    "        else:\n",
    "            ner_tag_list_fin.append([0,0])\n",
    "            tokens_fin.append(token)\n",
    "\n",
    "    example['ner_tag'] = ner_tag_list_fin\n",
    "    example['tokens'] = tokens_fin\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ccdf17",
   "metadata": {
    "cellId": "rla1zodjxltg4mu2ls641"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3359',\n",
       " 'text': 'The pizza is the best if you like thin crusted pizza.',\n",
       " 'aspectTerms': [{'term': 'pizza',\n",
       "   'polarity': 'positive',\n",
       "   'from': '4',\n",
       "   'to': '9'},\n",
       "  {'term': 'thin crusted pizza',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '34',\n",
       "   'to': '52'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}],\n",
       " 'ner_tag': [[0, 0],\n",
       "  [4, 5],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [0, 0],\n",
       "  [1, 7],\n",
       "  [2, 7],\n",
       "  [3, 7],\n",
       "  [0, 0]],\n",
       " 'tokens': ['The',\n",
       "  'pizza',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'if',\n",
       "  'you',\n",
       "  'like',\n",
       "  'thin',\n",
       "  'crusted',\n",
       "  'pizza',\n",
       "  '.']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "preprocess_text(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98f972c",
   "metadata": {
    "cellId": "g01q6j2zm9bgw6hw2s43de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_text at 0x7fc690dadfc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d887902df43c42b0b5b9f7bd623b690a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0e1ec7085545f4aec643169e287fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15970d21e5a41d8a3e3c0ae3766849d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentenceId', 'ner_tag', 'tokens'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "dataset = raw_datasets.map(\n",
    "    preprocess_text,\n",
    "    remove_columns = ['text', 'aspectTerms', 'aspectCategories']\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e72d3ba",
   "metadata": {
    "cellId": "deb0guhtiy9l1pfyd38u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three___________________________________[0, 0]\n",
      "courses_________________________________[4, 7]\n",
      "-_______________________________________[0, 0]\n",
      "choices_________________________________[0, 0]\n",
      "include_________________________________[0, 0]\n",
      "excellent_______________________________[0, 0]\n",
      "mussels_________________________________[4, 5]\n",
      ",_______________________________________[0, 0]\n",
      "puff____________________________________[1, 5]\n",
      "pastry__________________________________[2, 5]\n",
      "goat____________________________________[2, 5]\n",
      "cheese__________________________________[3, 5]\n",
      "and_____________________________________[0, 0]\n",
      "salad___________________________________[1, 5]\n",
      "with____________________________________[2, 5]\n",
      "a_______________________________________[2, 5]\n",
      "delicious_______________________________[2, 5]\n",
      "dressing________________________________[3, 5]\n",
      ",_______________________________________[0, 0]\n",
      "and_____________________________________[0, 0]\n",
      "a_______________________________________[0, 0]\n",
      "hanger__________________________________[1, 5]\n",
      "steak___________________________________[2, 5]\n",
      "au______________________________________[2, 5]\n",
      "poivre__________________________________[3, 5]\n",
      "that____________________________________[0, 0]\n",
      "is______________________________________[0, 0]\n",
      "out_____________________________________[0, 0]\n",
      "of______________________________________[0, 0]\n",
      "this____________________________________[0, 0]\n",
      "world___________________________________[0, 0]\n",
      "._______________________________________[0, 0]\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['train'][50]['tokens'], dataset['train'][50]['ner_tag']):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87defaea",
   "metadata": {
    "cellId": "jg9oj0c9cicmzdpm0ilh8"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984d6173",
   "metadata": {
    "cellId": "d364s0hvvcsxxeky83rwr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "269df13ae1894ac49949888803294fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca79d8964e8d47aeaa12b1c9aee9dd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d4a79fb95d4f7c8214a1a606259a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f15549b9db14caf9706bc5d2d4fb4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.1\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae881829",
   "metadata": {
    "cellId": "expakkar3ibvvwn6k0f5ws"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "label_count = len(label_list)\n",
    "def tokenize_and_align_labels(examples, label_all_tokens: bool = False):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tag\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append([-100 for l in range(label_count)])\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append([1 if l in label[word_idx] else 0 for l in range(label_count)])\n",
    "            else:\n",
    "                label_ids.append([1 if l in label[word_idx] else 0 for l in range(label_count)]\n",
    "                                     if label_all_tokens else [-100 for l in range(label_count)])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ab61877",
   "metadata": {
    "cellId": "fzr3senl4d2j3b1xuf86p"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e378dbe96b92483985f5b70e02b4d395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af5a373ca2341afae4c076bba672be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d6c4069b864f48aa1207157b74826f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    trial: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3041\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True,\n",
    "                               remove_columns = ['sentenceId', 'tokens', 'ner_tag'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c628983e",
   "metadata": {
    "cellId": "ii1wd6lxliiy9d3f9kuhs"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "tokenized_dataset.set_format(\n",
    "    \"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], output_all_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f58d56f",
   "metadata": {
    "cellId": "04jyv1cmls2y2eqkpej64dr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "They____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "did_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "not_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "have____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "may_____________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "##on____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "##nai___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "##se____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      ",_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "forgot__________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "our_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "toast___________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      ",_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "left____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "out_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "ingredients_____________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "(_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "i_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "##e_____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "cheese__________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "in______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "an______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "o_______________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "##mel___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "##et____________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      ")_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      ",_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "below___________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "hot_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "temperatures____________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "and_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "the_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "bacon___________________________________tensor([0, 0, 0, 0, 1, 0, 1, 0])\n",
      "was_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "so______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "over____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "cooked__________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "it______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "c_______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "##rum___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "##bled__________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n",
      "on______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "the_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "plate___________________________________tensor([0, 0, 0, 0, 1, 0, 0, 1])\n",
      "when____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "you_____________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "touched_________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "it______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "._______________________________________tensor([1, 0, 0, 0, 0, 0, 0, 0])\n",
      "[SEP]___________________________________tensor([-100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(tokenized_dataset['train'][10]['input_ids']), \n",
    "                        tokenized_dataset['train'][10]['labels']):\n",
    "    print(f'{token:_<40}{label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6ba17e",
   "metadata": {
    "cellId": "llzzbwc83qzia46vjrj6q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-25 18:14:55.554920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2eaa14",
   "metadata": {
    "cellId": "x5nq46rmc8dle6cp8h4qwq"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1da89392",
   "metadata": {
    "cellId": "cj3rmjbuum9r70qv20tit"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "class MultiLabelNERTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: Optional[torch.FloatTensor] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            class_weights = class_weights.to(self.args.device)\n",
    "            logging.info(f\"Using multi-label classification with class weights\", class_weights)\n",
    "        self.loss_fct = nn.BCEWithLogitsLoss(weight=class_weights)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        labels  = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # this accesses predictions for tokens that aren't CLS, PAD, or the 2nd+ subword in a word\n",
    "        # and simultaneously flattens the logits or labels\n",
    "        flat_outputs = outputs.logits[labels!=-100] \n",
    "        flat_labels  = labels[ labels!=-100]\n",
    "        \n",
    "        try:\n",
    "            loss = self.loss_fct(flat_outputs, flat_labels.float())\n",
    "        except AttributeError:  # DataParallel\n",
    "            loss = self.loss_fct(flat_outputs, flat_labels.float())\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be5db2f1",
   "metadata": {
    "cellId": "hqeqgvh90ddsv9d786dpbi"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "def compute_metrics(p, threshold=0.5):\n",
    "    predictions, labels = p\n",
    "\n",
    "    cleaned_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if [i for i in l if i != -100]]\n",
    "        for prediction, label in zip(predictions, labels)]\n",
    "    true_predictions = []\n",
    "    for prediction in cleaned_predictions:\n",
    "        temp = sigmoid(torch.Tensor(prediction))\n",
    "        y_pred = np.zeros(temp.shape)\n",
    "        y_pred[np.where(temp >= 0.5)] = 1\n",
    "        true_predictions.extend(y_pred)\n",
    "    \n",
    "    cleaned_labels = [\n",
    "    [l for (p, l) in zip(prediction, label) if [i for i in l if i != -100]]\n",
    "    for prediction, label in zip(predictions, labels)]\n",
    "    true_labels = []\n",
    "    for label in cleaned_labels:\n",
    "        temp = np.array(label)\n",
    "        true_labels.extend(temp)\n",
    "        \n",
    "    f1_micro_average = f1_score(y_true=true_labels, y_pred=true_predictions, average='macro')\n",
    "    roc_auc = roc_auc_score(true_labels, true_predictions, average = 'macro')\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    \n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5edbcd1",
   "metadata": {
    "cellId": "r78bsvqrd1n2nttuhwgi33"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a2b5adb9d141f3b8743f912866d460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21287' max='21287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21287/21287 27:45, Epoch 7/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.051822</td>\n",
       "      <td>0.697294</td>\n",
       "      <td>0.832431</td>\n",
       "      <td>0.929171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.045367</td>\n",
       "      <td>0.733207</td>\n",
       "      <td>0.855390</td>\n",
       "      <td>0.937094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.043288</td>\n",
       "      <td>0.794379</td>\n",
       "      <td>0.869353</td>\n",
       "      <td>0.950008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.044566</td>\n",
       "      <td>0.821954</td>\n",
       "      <td>0.894868</td>\n",
       "      <td>0.955158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.050844</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.892175</td>\n",
       "      <td>0.956584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.054592</td>\n",
       "      <td>0.813300</td>\n",
       "      <td>0.885446</td>\n",
       "      <td>0.956663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.054683</td>\n",
       "      <td>0.813375</td>\n",
       "      <td>0.885847</td>\n",
       "      <td>0.956821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=21287, training_loss=0.033653985380471194, metrics={'train_runtime': 1666.7125, 'train_samples_per_second': 12.772, 'train_steps_per_second': 12.772, 'total_flos': 760931237937024.0, 'train_loss': 0.033653985380471194, 'epoch': 7.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-large-cased\", problem_type=\"multi_label_classification\",\n",
    "    num_labels=8, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"token_class_model\",\n",
    "    learning_rate=5e-06,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    save_strategy=\"no\", \n",
    "    group_by_length=True,\n",
    "    warmup_ratio=0.1,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = MultiLabelNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2b2d8e6",
   "metadata": {
    "cellId": "tijwo1374uoq19pxor00r8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!g2.1\n",
    "trial = trainer.predict(tokenized_dataset['trial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a66d90b6",
   "metadata": {
    "cellId": "iyvnyxuagh7vzdgkgjq3f"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "preds = trial.predictions[0]\n",
    "label = trial.label_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2af728c0",
   "metadata": {
    "cellId": "4ya89apcn5ldcfddlmd4hi"
   },
   "outputs": [],
   "source": [
    "#!g2.1\n",
    "def return_labels(preds, label):   \n",
    "    all_preds = []\n",
    "    for pred, lbl in zip(preds, label):\n",
    "        true_pred = [p for (p,l) in zip(pred, lbl) if l != -100]\n",
    "        if len(true_pred) > 0:\n",
    "            temp = sigmoid(torch.Tensor(true_pred))\n",
    "            y_pred = np.zeros(temp.shape)\n",
    "            y_pred[np.where(temp >= 0.5)] = 1\n",
    "            all_preds.append(y_pred.tolist())\n",
    "    \n",
    "    tags = []\n",
    "    for token in all_preds:\n",
    "        if sum(i for i in token)> 1:\n",
    "            temp = []\n",
    "            for indx, element in enumerate(token):\n",
    "                #temp = []\n",
    "                if element !=0:\n",
    "                    temp.append(id2label[indx])\n",
    "            tags.append(temp)\n",
    "        else:\n",
    "            for indx, element in enumerate(token):\n",
    "                if element !=0:\n",
    "                    tags.append(id2label[indx])\n",
    "    return tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d345f0c",
   "metadata": {
    "cellId": "ttxeiumxmpaajt0ivsg4g7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '1609',\n",
       " 'text': 'Service was quick.',\n",
       " 'aspectTerms': [{'term': 'Service',\n",
       "   'polarity': 'positive',\n",
       "   'from': '0',\n",
       "   'to': '7'}],\n",
       " 'aspectCategories': [{'category': 'service', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18637a47",
   "metadata": {
    "cellId": "l8le19t53apv0zi4ppxp0g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service_________________________________['S', 'POS']\n",
      "was_____________________________________O\n",
      "quick___________________________________O\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][5]['tokens'], return_labels(trial.predictions[5], trial.label_ids[5])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27e25711",
   "metadata": {
    "cellId": "5unlebfo95ivxbvfba0sn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3041',\n",
       " 'text': \"We've only eaten in the restaurant once, but we have ordered many times for dinner.\",\n",
       " 'aspectTerms': [{'term': 'dinner',\n",
       "   'polarity': 'neutral',\n",
       "   'from': '76',\n",
       "   'to': '82'}],\n",
       " 'aspectCategories': [{'category': 'anecdotes/miscellaneous',\n",
       "   'polarity': 'neutral'}]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abbe7fd6",
   "metadata": {
    "cellId": "65dayhndwu6c1aros8407v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We______________________________________O\n",
      "'ve_____________________________________O\n",
      "only____________________________________O\n",
      "eaten___________________________________O\n",
      "in______________________________________O\n",
      "the_____________________________________O\n",
      "restaurant______________________________O\n",
      "once____________________________________O\n",
      ",_______________________________________O\n",
      "but_____________________________________O\n",
      "we______________________________________O\n",
      "have____________________________________O\n",
      "ordered_________________________________O\n",
      "many____________________________________O\n",
      "times___________________________________O\n",
      "for_____________________________________O\n",
      "dinner__________________________________['S', 'NEU']\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][99]['tokens'], return_labels(trial.predictions[99], trial.label_ids[99])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b175518",
   "metadata": {
    "cellId": "vnmijp6lc1wbc1qqoilu"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentenceId': '3170',\n",
       " 'text': 'Good spreads, great beverage selections and bagels really tasty.',\n",
       " 'aspectTerms': [{'term': 'spreads',\n",
       "   'polarity': 'positive',\n",
       "   'from': '5',\n",
       "   'to': '12'},\n",
       "  {'term': 'beverage selections',\n",
       "   'polarity': 'positive',\n",
       "   'from': '20',\n",
       "   'to': '39'},\n",
       "  {'term': 'bagels', 'polarity': 'positive', 'from': '44', 'to': '50'}],\n",
       " 'aspectCategories': [{'category': 'food', 'polarity': 'positive'}]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g2.1\n",
    "raw_datasets['trial'][40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2183be89",
   "metadata": {
    "cellId": "e9ujs6xw94i6z0g1tybij6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good____________________________________O\n",
      "spreads_________________________________['S', 'POS']\n",
      ",_______________________________________O\n",
      "great___________________________________O\n",
      "beverage________________________________['B', 'POS']\n",
      "selections______________________________['E', 'POS']\n",
      "and_____________________________________O\n",
      "bagels__________________________________['S', 'POS']\n",
      "really__________________________________O\n",
      "tasty___________________________________O\n",
      "._______________________________________O\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag in zip(dataset['trial'][40]['tokens'], return_labels(trial.predictions[40], trial.label_ids[40])):\n",
    "    print(f'{token:_<40}{ner_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2cacba9f",
   "metadata": {
    "cellId": "tz5t53rud9cl47s4a9u2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good____________________________________O [0, 0]\n",
      "spreads_________________________________['S', 'POS'] [4, 5]\n",
      ",_______________________________________O [0, 0]\n",
      "great___________________________________O [0, 0]\n",
      "beverage________________________________['B', 'POS'] [1, 5]\n",
      "selections______________________________['E', 'POS'] [3, 5]\n",
      "and_____________________________________O [0, 0]\n",
      "bagels__________________________________['S', 'POS'] [4, 5]\n",
      "really__________________________________O [0, 0]\n",
      "tasty___________________________________O [0, 0]\n",
      "._______________________________________O [0, 0]\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "for token, ner_tag, label in zip(dataset['trial'][40]['tokens'], return_labels(trial.predictions[40], trial.label_ids[40]),\\\n",
    "                                 dataset['trial'][40]['ner_tag']):\n",
    "    print(f'{token:_<40}{ner_tag} {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5e7599d",
   "metadata": {
    "cellId": "l5lpmg8awkfg4ajx5csa26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on E2E ABSA: 0.7214640198511166\n"
     ]
    }
   ],
   "source": [
    "#!g2.1\n",
    "trial = trainer.predict(tokenized_dataset['test'])\n",
    "\n",
    "predictions = []\n",
    "for i in range(len(trial.predictions)):\n",
    "    pred = return_labels(trial.predictions[i], trial.label_ids[i])\n",
    "    predictions.append(pred)\n",
    "\n",
    "count_total = 0\n",
    "count_matches = 0\n",
    "for label, prediction in zip(dataset['test']['ner_tag'],predictions):\n",
    "    for lbl, pred in zip(label, prediction):\n",
    "        if lbl != [0,0]:\n",
    "            count_total += 1\n",
    "            new_label = []\n",
    "            for item in lbl:\n",
    "                \n",
    "                new_item = id2label[item]\n",
    "                new_label.append(new_item)\n",
    "            if new_label == pred:\n",
    "                count_matches += 1\n",
    "print(f'Accuracy on E2E ABSA: {count_matches/count_total}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Финальные выводы:\n",
    "\n",
    "Результат классификации токенов получился немного лучше, чем при одномерных тегах, но точность извлечения и классификации токена не сильно поменялась. В следующем эксперименте выберу другоую модель за основу - Albert - и посмотрю, как это подвлияет на результат"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "deb93d5a-f933-46be-9267-d56b3eaed0ef",
  "notebookPath": "Multilable_token_classif.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
